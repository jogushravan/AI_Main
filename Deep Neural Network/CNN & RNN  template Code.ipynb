{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850db3f6",
   "metadata": {},
   "source": [
    "RNN are good for sequential data processing — NLP tasks and time-series predictions.\n",
    "\n",
    "A typical RNN for NLP architecture is an embedding layer (pretrained or not) and a sequence of bidirectional LSTM layers since all text is visible to the model immediately.\n",
    "RNN for time-series predictions is typically one-directional, although using bidirectional layers may improve the quality too. 1D convolutions and other techniques and tricks are also widely used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8700f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN for NLP\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size,\n",
    "                     embedding_dim,\n",
    "                     input_length=max_length),\n",
    "    layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(16)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(6, activation='softmax')  # multiclass classification\n",
    "])\n",
    "\n",
    "# RNN for time series\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(filters=128, kernel_size=7,\n",
    "                  strides=1, padding=\"causal\",\n",
    "                  activation=\"relu\",\n",
    "                  input_shape=(WINDOW_SIZE, 1)),\n",
    "    # univariate time series - predict a value based on\n",
    "    # 'WINDOW_SIZE' previous steps for 1 feature\n",
    "\n",
    "    layers.LSTM(32, return_sequences=True),\n",
    "    layers.LSTM(16, return_sequences=True),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1)  # predict one value\n",
    "])\n",
    "\n",
    "# explore your model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c1619c",
   "metadata": {},
   "source": [
    "More Complex Neural Network Architectures with Functional API\n",
    "\n",
    "Functional API allows you to add non-sequential connections in your network, and specify multiple inputs and outputs. Below is a rather cumbersome, but demonstrative description of a network that takes text, an image, and several numbers as input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable-size input for text\n",
    "text_input = tf.keras.Input(shape=(None,), name=\"text_input\")\n",
    "\n",
    "# input for images IMG_HEIGHT x IMG_WEGTH x CHANNELS\n",
    "image_input = tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS),\n",
    "                             name=\"image_input\")\n",
    "\n",
    "# input for numbers ('num_inputs' variables)\n",
    "numbers_input = tf.keras.Input(shape=(num_inputs,), name=\"numbers_input\")\n",
    "\n",
    "# processing text\n",
    "text_features = layers.Embedding(max_length,\n",
    "                                 embedding_dim,\n",
    "                                 input_length=max_length)(text_input)\n",
    "text_features_buf = layers.Bidirectional(\n",
    "    layers.LSTM(64, return_sequences=True))(text_features)\n",
    "text_features = layers.Bidirectional(layers.LSTM(\n",
    "    64, return_sequences=True))(text_features_buf)\n",
    "text_features = layers.Bidirectional(\n",
    "    layers.LSTM(64, return_sequences=True))(text_features)\n",
    "text_features = layers.Add()([text_features, text_features_buf])\n",
    "text_features = layers.LayerNormalization()(text_features)\n",
    "text_features = layers.Bidirectional(layers.LSTM(32))(text_features)\n",
    "\n",
    "# processing images\n",
    "image_features = layers.Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "image_features = layers.MaxPooling2D(2, 2)(image_features)\n",
    "image_features = layers.Conv2D(32, (3, 3), activation='relu')(image_features)\n",
    "image_features = layers.MaxPooling2D(2, 2)(image_features)\n",
    "image_features = layers.Flatten()(image_features)\n",
    "\n",
    "# processing numbers\n",
    "numbers_features = layers.Dense(units=64, activation='relu')(numbers_input)\n",
    "numbers_features_buf = layers.Dense(\n",
    "    units=32, activation='relu')(numbers_features)\n",
    "numbers_features = layers.Dense(\n",
    "    units=32, activation='relu')(numbers_features_buf)\n",
    "numbers_features = layers.Dense(units=32, activation='relu')(numbers_features)\n",
    "numbers_features = layers.Add()([numbers_features, numbers_features_buf])\n",
    "numbers_features = layers.LayerNormalization()(numbers_features)\n",
    "\n",
    "# merge all available features into a single large vector\n",
    "concat = layers.concatenate([text_features,\n",
    "                             image_features,\n",
    "                             numbers_features])\n",
    "\n",
    "intermediate_output = tf.keras.layers.Dense(64, activation='relu')(concat)\n",
    "\n",
    "# final part of the model\n",
    "dense_1 = layers.Dense(64, activation='relu')(concat)\n",
    "dropout_1 = layers.Dropout(0.2)(dense_1)\n",
    "dense_2 = layers.Dense(32, activation='relu')(dropout_1)\n",
    "dropout_2 = layers.Dropout(0.2)(dense_2)\n",
    "output = layers.Dense(1, activation='relu')(dropout_2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input, image_input, numbers_input],\n",
    "                       outputs=[intermediate_output, output])\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True,\n",
    "           show_layer_names=False, to_file='data/img/model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492959eb",
   "metadata": {},
   "source": [
    "Preparing Data for Computer Vision and NLP tasks\n",
    "\n",
    "Using ImageDataGenerator\n",
    "ImageDataGenerator will help you a lot in computer vision tasks —it will label your images automatically based on directory structure or perform in-memory data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8396fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# default ImageDataGenerator without augmentation\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'  # 'categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'  # 'categorical'\n",
    ")\n",
    "\n",
    "# Another option is not to specify the validation set explicitly,\n",
    "# but to entrust the split to ImageDataGenerator\n",
    "# In this case, be sure that the seed parameter is the same\n",
    "# for both sets, otherwise the sets may overlap\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   # set validation split ratio\n",
    "                                   validation_split=0.2)  \n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',  # 'categorical'\n",
    "    subset='training',  # set as training data\n",
    "    seed=SEED)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # the same directory\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',  # 'categorical'\n",
    "    subset='validation',  # set as validation data\n",
    "    seed=SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c7fc028",
   "metadata": {},
   "source": [
    "Tokenizing and padding sentences for NLP tasks\n",
    "\n",
    "Tokenization and padding sentences are common practices for NLP tasks. First, you turn sentences into vectors, and then you make sure that all these vectors have a fixed length to feed them to the input of your model.\n",
    "\n",
    "![title](\"RNN Tokenizer anbd Pd Seq.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default parameters\n",
    "vocab_size = 1000\n",
    "embedding_dim = 32\n",
    "max_length = 100\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\"\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences,\n",
    "                                maxlen=max_length,\n",
    "                                padding=padding_type,\n",
    "                                truncating=trunc_type)\n",
    "\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
    "validation_padded = pad_sequences(validation_sequences,\n",
    "                                  maxlen=max_length,\n",
    "                                  padding=padding_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
