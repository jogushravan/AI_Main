{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496989c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorFlow/Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa0b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Importing all Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# special imports for CV and NLP\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(tf.__version__)\n",
    "# 2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc39433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Typical Neural Network Architectures with Sequential API\n",
    "# 3 main architectures of neural networks are deep feed-forward\n",
    "        #Deep Nueral Network, convolutional and recurrent nets:\n",
    "\n",
    "#1 DNN:  simple sequence of fully connected (dense) layers + dropout, batch normalization\n",
    "#2 CNN:image classification, object detection, semantic segmentation, and other computer vision tasks\n",
    "#    CNN architecture is a sequence of convolutional-pooling blocks followed by a little fully connected network\n",
    "#3 RNN: sequential data processing â€” NLP tasks and time-series predictions\n",
    "#    NLP architecture is an embedding layer and a sequence of bidirectional LSTM layers\n",
    "#    RNN for time-series predictions is typically one-directional, although using bidirectional layers may improve the quality too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu',\n",
    "                          input_shape=(num_inputs, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=1, activation='relu'),  # regression\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e32ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "model = tf.keras.models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                  input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')  # binary classification\n",
    "    # multi-class classification\n",
    "    # layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN for NLP\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size,\n",
    "                     embedding_dim,\n",
    "                     input_length=max_length),\n",
    "    layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(16)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(6, activation='softmax')  # multiclass classification\n",
    "])\n",
    "\n",
    "# RNN for time series\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(filters=128, kernel_size=7,\n",
    "                  strides=1, padding=\"causal\",\n",
    "                  activation=\"relu\",\n",
    "                  input_shape=(WINDOW_SIZE, 1)),\n",
    "    # univariate time series - predict a value based on\n",
    "    # 'WINDOW_SIZE' previous steps for 1 feature\n",
    "\n",
    "    layers.LSTM(32, return_sequences=True),\n",
    "    layers.LSTM(16, return_sequences=True),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1)  # predict one value\n",
    "])\n",
    "\n",
    "# explore your model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model 3 PARAMETERS: optimizer, the loss function, and the metrics \n",
    "#optimizer : Adam or RMSProp\n",
    "#Loss function \n",
    "#MSE, MAE, or Huber loss for regression,\n",
    "#binary cross-entropy loss for binary classification, and sparse categorical cross-entropy loss for multiclass classification tasks if your label is an integer (i.e. 1, 2, 3 for 3-class classification).\n",
    "#Categorical cross-entropy loss is usually used when your labels are represented as one-hot encoded vectors (i.e. [1, 0, 0], [0, 1, 0] and [0, 0, 1] for 3-class classification).\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Typical optimizers:\n",
    "# - tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# - tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "# Typical classification losses:\n",
    "# - tf.keras.losses.BinaryCrossentropy()\n",
    "# - tf.keras.losses.CategoricalCrossentropy()        # y - one-hot\n",
    "# - tf.keras.losses.SparseCategoricalCrossentropy()  # y - integer\n",
    "\n",
    "# Typical regression losses:\n",
    "# - tf.keras.losses.MeanSquaredError()\n",
    "# - tf.keras.losses.MeanAbsoluteError()\n",
    "# - tf.keras.losses.Huber()\n",
    "\n",
    "# Typical metrics\n",
    "# - ['accuracy'] # for classification\n",
    "# - ['mse', 'mae'] # for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9348f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    " #1 regular data in the form of arrays or tensors\n",
    " #2 ImageDataGenerator for computer vision\n",
    " #3  padded sequences for NLP\n",
    "    \n",
    "    #CALBACKS \n",
    "        #ModelCheckpoint (saves your model during its training)\n",
    "        #EarlyStopping (stops the training process if the loss has stopped improving)\n",
    "# callbacks\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='data/model_callback.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',  # 'val_accuracy'\n",
    "    mode='min',  # 'max'\n",
    "    save_best_only=True,\n",
    "    verbose=1)\n",
    "\n",
    "earlystop_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "callbacks = [earlystop_cb, checkpoint_cb]\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# model.fit\n",
    "\n",
    "# from from regular array data - see DNN architecture\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_split=0.1,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=callbacks,  # using callbacks\n",
    "    verbose=1)\n",
    "\n",
    "# from ImageDataGenerator - see CNN architecture\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    verbose=1)\n",
    "\n",
    "# from padded sequences - see RNN for NLP architecture\n",
    "history = model.fit(\n",
    "    training_padded,\n",
    "    training_labels,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(validation_padded, validation_labels),\n",
    "    verbose=1)\n",
    "\n",
    "# from tf.data.Dataset - see RNN for time series architecture\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f488ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save  training results into a history variable to explore learning curves\n",
    "# code is taken from:\n",
    "# https://github.com/https-deeplearning-ai/tensorflow-1-public\n",
    "\n",
    "def plot_learning_curves(history, parameter):\n",
    "    \"\"\" \n",
    "    Plot learning curves from 'history' object.\n",
    "\n",
    "    Parameters:\n",
    "        history (History object): result of the `fit` method execution\n",
    "        parameter (str): parameter to explore: 'accuracy', 'loss', etc.\n",
    "\n",
    "    >>> history = model.fit(...)\n",
    "    >>> plot_learning_curves(history, \"loss\")\n",
    "    >>> plot_learning_curves(history, \"accuracy\")\n",
    "    >>> plot_learning_curves(history, \"mse\")\n",
    "    \"\"\"\n",
    "\n",
    "    plt.plot(history.history[parameter])\n",
    "    plt.plot(history.history['val_'+parameter])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(parameter)\n",
    "    plt.legend([parameter, 'val_'+parameter])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_learning_curves(history, \"loss\")\n",
    "plot_learning_curves(history, \"accuracy\")\n",
    "plot_learning_curves(history, \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c2f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model --> model performance on previously unseen data\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58aed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save and load the model\n",
    "model_path = 'data/model.h5'\n",
    "model.save(model_path)\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# save using current date and time in filename\n",
    "import time\n",
    "current_time = time.strftime(\"%Y_%m_%d-%H_%M_%S\")  # 2022_02_01-14_05_32\n",
    "model_path = f'data/model_{current_time}.h5'\n",
    "model.save(model_path)\n",
    "\n",
    "# endddddddddddddd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
